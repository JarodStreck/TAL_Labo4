{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 4<br/>Reconnaissance des entités nommées\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce labo est de comparer la reconnaissance des entités nommées (*named entity recognition*, NER) faite par spaCy avec celle faite par NLTK, sur des données en anglais fournies sur Cyberlearn.  Veuillez fournir les scores de rappel, précision et F1-score pour chacun des tags présents dans les données de test.  Veuillez comparer deux modèles de spaCy, 'en_core_web_sm' et 'en_core_web_lg'.\n",
    "\n",
    "Vous pouvez concevoir l'ensemble du projet par vous-mêmes, ou suivre les indications suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NER avec spaCy et NLTK sur un texte court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm \n",
    "# exécuter la ligne ci-dessus une fois, si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Reinhold Messner made a solo ascent of Mount Everest and was later a member of the European Parliament.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Veuillez traiter ce texte avec la pipeline 'nlp', et pour chaque entité nommée trouvée veuillez afficher les mots qui la composent et son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinhold Messner [PERSON]\n",
      "Mount Everest [LOC]\n",
      "the European Parliament [ORG]\n"
     ]
    }
   ],
   "source": [
    "docs = nlp(raw_text)\n",
    "\n",
    "for ent in docs.ents:\n",
    "    print(ent.text + \" [\" + ent.label_ + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('maxent_ne_chunker') \n",
    "#nltk.download('words') \n",
    "# exécuter les deux lignes ci-dessus une fois, si nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Veuillez effectuer avec NLTK la tokenization, le POS tagging et le *NE chunking* (voir la [documentation NLTK](https://www.nltk.org/api/nltk.chunk.ne_chunk.html#nltk.chunk.ne_chunk)).  Veuillez afficher le résultat et indiquer son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "pos_tagged_tokens = nltk.pos_tag(tokens)    \n",
    "ne_chunks = nltk.ne_chunk(pos_tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Veuillez afficher, pour chaque entité nommée, les mots qui la composent et son type.  Vous pouvez parcourir le résultat précédent avec une boucle for, et déterminer si un noeud a une étiquette avec la fonction `hasattr(noeud, 'label')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Reinhold/NNP)\n",
      "(PERSON Messner/NNP)\n",
      "(PERSON Mount/NNP Everest/NNP)\n",
      "(ORGANIZATION European/NNP Parliament/NNP)\n"
     ]
    }
   ],
   "source": [
    "for ne_chunk in ne_chunks:\n",
    "    if hasattr(ne_chunk, 'label'):\n",
    "        print(ne_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** À ce stade, que pensez-vous de la qualité des résultats de chaque système ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur cet phrase en particulier, nlp s'en sort bien mieux, il a fait tous juste.\n",
    "# Il a identifié \"Reinhold Messner\" comme une personne, le mont everest comme une location et le parlement européen comme une organisation.\n",
    "# Alors que nltk a identifié \"Reinhold Messner\" comme 2 personnes différentes, le mont everest comme une personne. Il a juste fait juste le parlement européen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données de test\n",
    "\n",
    "**2a.** Quel est le format du fichier `eng.test.a.conll` ?  Quelle information contient chaque colonne ?  Quel est le format des tags NE ?\n",
    "\n",
    "Note : ce fichier fait partie des données de test pour la NER sur l'anglais de la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003. On peut lire [ici](https://www.clips.uantwerpen.be/conll2003/ner/) la description de la tâche et les scores obtenus.  On peut trouver une copie des données [ici](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) ou [ici](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format IO (pas de beginning)\n",
    "# Colonne 1 : Le mot\n",
    "# Colonne 2 : Le POS tag\n",
    "# Colonne 3 : Le chunk NE\n",
    "# Colonne 4 : Le label NE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez charger les données de `eng.test.a.conll` grâce à la classe `ConllCorpusReader` de NLTK vue dans les labos précédents (voir [documentation](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader)). Veuillez lire les colonnes qui contiennent les tokens ('words'), les POS tags ('pos') et les informations sur les entités nommées ('chunk') et afficher les trois premières phrases, accessibles via la méthode `.iob_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('CRICKET', 'NNP', 'O'), ('-', ':', 'O'), ('LEICESTERSHIRE', 'NNP', 'I-ORG'), ('TAKE', 'NNP', 'O'), ('OVER', 'IN', 'O'), ('AT', 'NNP', 'O'), ('TOP', 'NNP', 'O'), ('AFTER', 'NNP', 'O'), ('INNINGS', 'NNP', 'O'), ('VICTORY', 'NN', 'O'), ('.', '.', 'O')]\n",
      "[('LONDON', 'NNP', 'I-LOC'), ('1996-08-30', 'CD', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "\n",
    "corpus = ConllCorpusReader('.', 'eng.test.a.conll',columntypes=['words', 'pos', 'ignore','chunk'])\n",
    "\n",
    "sents = corpus.iob_sents()\n",
    "print(sents[0])\n",
    "print(sents[1])\n",
    "print(sents[2])\n",
    "\n",
    "#TODO pas sur pourquoi mais y'a rien dans la première phrase (pas d'entitee nomee dans la 1ere phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Veuillez préparer les données pour le test, en ne gardant que les phrases ayant au moins trois (3) tokens (pas 0, 1, 2) :\n",
    "\n",
    "* une variable `test_tokens` contiendra les tokens groupés par phrase (liste de listes de strings)\n",
    "* une variable `test_tags` contiendra les tags NE en une seule liste (en vue de l'évaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "50817\n"
     ]
    }
   ],
   "source": [
    "sents = list(sents)\n",
    "sents = list(filter(lambda x: len(x) > 2, sents))\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "for sent in sents:\n",
    "    curr_tokens = []\n",
    "    for token in sent:\n",
    "       curr_tokens.append(token[0])\n",
    "       test_tags.append(token[2])\n",
    "    test_tokens.append(curr_tokens)\n",
    "\n",
    "print(test_tokens[0])\n",
    "print(len(test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Combien d'occurrences de tags contient `test_tags`?  Combien de tags différents y a-t-il, et lesquels sont-ils ?  Combien il y a d'occurrences de tags de chaque type ?  Combien de phrases y a-t-il dans `test_tokens` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases dans test_tokens :  2970\n",
      "Nombre de tags dans test_tags :  50817\n",
      "Nombre de tags uniques :  5\n",
      "List des tags :  {'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}\n",
      "Nombre d'occurence de O 42474\n",
      "Nombre d'occurence de I-PER 3097\n",
      "Nombre d'occurence de I-LOC 1938\n",
      "Nombre d'occurence de I-MISC 1228\n",
      "Nombre d'occurence de I-ORG 2080\n"
     ]
    }
   ],
   "source": [
    "unique_data = set(test_tags)\n",
    "print(\"Nombre de phrases dans test_tokens : \",len(test_tokens))\n",
    "print(\"Nombre de tags dans test_tags : \",len(test_tags)) \n",
    "print(\"Nombre de tags uniques : \",len(unique_data))\n",
    "print(\"List des tags : \" , unique_data)\n",
    "print(\"Nombre d'occurence de O\",test_tags.count(\"O\"))\n",
    "print(\"Nombre d'occurence de I-PER\",test_tags.count(\"I-PER\"))\n",
    "print(\"Nombre d'occurence de I-LOC\",test_tags.count(\"I-LOC\"))\n",
    "print(\"Nombre d'occurence de I-MISC\",test_tags.count(\"I-MISC\"))\n",
    "print(\"Nombre d'occurence de I-ORG\",test_tags.count(\"I-ORG\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performances de NLTK pour la NER\n",
    "\n",
    "**3a.** Le NER de NLTK a un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertir chaque tag du NER de NLTK dans le tag correspondant pour les données de test.  Attention à la logique des conversions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nltk_conll(nltk_tag):\n",
    "   translated_tags = []\n",
    "   corresp = { 'O':'O',\n",
    "               'ORGANIZATION':'I-ORG',\n",
    "               'PERSON':'I-PER',\n",
    "               'LOCATION':'I-LOC',\n",
    "               'FACILITY': 'I-LOC',\n",
    "               'GPE': 'I-LOC'\n",
    "             }\n",
    "   for tag in nltk_tag:\n",
    "      if tag in corresp:\n",
    "        translated_tags.append(corresp[tag])\n",
    "      else:\n",
    "        translated_tags.append('I-MISC')  # Les entités qui ne sont pas dans notre fichier test seron aussi considérés comme des entités nommées de type MISC\n",
    "   return translated_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez exécuter la NER de NLTK sur chacune des phrases de `test_tokens`, ce qui assure que NLTK aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `nltk_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tags = []\n",
    "\n",
    "for sent in test_tokens:\n",
    "    pos_tagged_tokens = nltk.pos_tag(sent) \n",
    "    ne_chunks = nltk.ne_chunk(pos_tagged_tokens)\n",
    "    for ne_chunk in ne_chunks:\n",
    "        if hasattr(ne_chunk, 'label'):\n",
    "            for leaf in ne_chunk.leaves():\n",
    "                nltk_tags.append(ne_chunk.label())\n",
    "        else:\n",
    "            nltk_tags.append('O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Veuillez convertir les tags de `nltk_tags` grâce à la fonction précédente, dans une liste appelée `nltk_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tags:  50817\n",
      "10 premiers tags:  ['I-LOC', 'O', 'I-ORG', 'O', 'O', 'O', 'I-ORG', 'O', 'I-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "nltk_tags_conv = convert_nltk_conll(nltk_tags)\n",
    "print(\"Nombre de tags: \", len(nltk_tags_conv))\n",
    "print(\"10 premiers tags: \" , nltk_tags_conv[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport d'évaluation de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-LOC       0.72      0.55      0.62      2561\n",
      "      I-MISC       0.01      0.16      0.01        45\n",
      "       I-ORG       0.35      0.51      0.41      1429\n",
      "       I-PER       0.74      0.77      0.76      3006\n",
      "           O       0.99      0.96      0.97     43776\n",
      "\n",
      "    accuracy                           0.91     50817\n",
      "   macro avg       0.56      0.59      0.56     50817\n",
      "weighted avg       0.94      0.91      0.93     50817\n",
      "\n",
      "Matrice de confusion :\n",
      "[[ 1396   511   253   160   241]\n",
      " [   29     7     5     2     2]\n",
      " [  214   187   728   156   144]\n",
      " [  162    85   377  2306    76]\n",
      " [  137   438   717   473 42011]]\n"
     ]
    }
   ],
   "source": [
    "# Afficher le rapport d'évaluation de classification\n",
    "print(\"Rapport d'évaluation de classification :\")\n",
    "print(classification_report(nltk_tags_conv, test_tags))\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "print(\"Matrice de confusion :\")\n",
    "print(confusion_matrix(nltk_tags_conv, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performances de spaCy pour la NER\n",
    "\n",
    "**4a.** Le NER de spaCy a aussi un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertir chaque tag du NER de spaCy dans le tag correspondant pour les données de test.  Attention à la logique des conversions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O', 'I-MISC', 'I-PER', 'I-LOC', 'I-ORG'}\n",
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
     ]
    }
   ],
   "source": [
    "# Recherche des tags qui diffèrent\n",
    "\n",
    "# Tags de spacy\n",
    "ner_labels = nlp.get_pipe(\"ner\").labels\n",
    "\n",
    "# Tags de la notre fichier de test\n",
    "set_tags = set()\n",
    "for sent in sents:\n",
    "    for token in sent:\n",
    "        set_tags.add(token[2])\n",
    "\n",
    "print(set_tags)\n",
    "print(ner_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spacy_conll(spacy_tag):\n",
    "  corresp = { 'O':'O',\n",
    "              'ORG':'I-ORG',\n",
    "              'PERSON':'I-PER',\n",
    "              'LOC':'I-LOC',\n",
    "              'FAC': 'I-LOC',\n",
    "              'GPE': 'I-LOC'\n",
    "            }\n",
    "  translated_tags = []\n",
    "  for tag in spacy_tag:\n",
    "    if tag in corresp:\n",
    "      translated_tags.append(corresp[tag])\n",
    "    else:\n",
    "      translated_tags.append('I-MISC')\n",
    "    \n",
    "\n",
    "  return translated_tags\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez exécuter la NER de spaCy sur chacune des phrases de `test_tokens`, ce qui assure que spaCy aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `spacy_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_ner(nlp, tokens):\n",
    "    spacy_tags = []\n",
    "    for sent in test_tokens:\n",
    "        # Traiter la phrase avec spaCy\n",
    "        string_sent = ' '.join(sent)\n",
    "        doc = nlp(string_sent) # Meilleur resultat si on analyze la phrase entiere\n",
    "        for token in sent:\n",
    "            found = False\n",
    "            # La tokenisatin est differente dans spacy. Pas la methode la plus optimale mais il faut faire\n",
    "            # coorespondre les tags de spacy avec les tags de notre fichier de test.\n",
    "            # (par example West indian n'est pas reconnu si on analyse les tokens un par un)\n",
    "            for ent in doc.ents: \n",
    "                if token in ent.text:\n",
    "                    spacy_tags.append(ent.label_)\n",
    "                    found = True\n",
    "                    break\n",
    "            if (not found):\n",
    "                spacy_tags.append(\"O\")\n",
    "    return spacy_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tags = spacy_ner(nlp, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez convertir les tags de `spacy_tags` grâce à la fonction précédente, dans une liste appelée `spacy_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tags:  50817\n",
      "10 premiers tags:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "spacy_tags_conv = convert_spacy_conll(spacy_tags)\n",
    "print(\"Nombre de tags: \", len(spacy_tags_conv))\n",
    "print(\"10 premiers tags: \" , spacy_tags_conv[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport d'évaluation de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-LOC       0.79      0.62      0.69      2447\n",
      "      I-MISC       0.66      0.12      0.20      6813\n",
      "       I-ORG       0.51      0.44      0.47      2436\n",
      "       I-PER       0.73      0.73      0.73      3062\n",
      "           O       0.82      0.96      0.88     36059\n",
      "\n",
      "    accuracy                           0.79     50817\n",
      "   macro avg       0.70      0.57      0.60     50817\n",
      "weighted avg       0.77      0.79      0.75     50817\n",
      "\n",
      "Matrice de confusion :\n",
      "[[ 1522    57   221    69   578]\n",
      " [   48   805    39    36  5885]\n",
      " [  134   129  1065   241   867]\n",
      " [   47    29   226  2250   510]\n",
      " [  187   208   529   501 34634]]\n"
     ]
    }
   ],
   "source": [
    "# Afficher le rapport d'évaluation de classification\n",
    "print(\"Rapport d'évaluation de classification :\")\n",
    "print(classification_report(spacy_tags_conv, test_tags))\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "print(\"Matrice de confusion :\")\n",
    "print(confusion_matrix(spacy_tags_conv, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e.** Veuillez exécuter également le modèle 'en_core_web_lg' de spacy et afficher le rapport d'évaluation (mais pas la matrice de confusion).  Vous pouvez recopier ici le minimum de code nécessaire à l'obtention des résultats, avec une nouvelle pipeline spaCy appelée 'nlp2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport d'évaluation de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-LOC       0.83      0.61      0.70      2651\n",
      "      I-MISC       0.68      0.12      0.21      6956\n",
      "       I-ORG       0.58      0.48      0.52      2488\n",
      "       I-PER       0.84      0.79      0.82      3278\n",
      "           O       0.81      0.97      0.89     35444\n",
      "\n",
      "    accuracy                           0.80     50817\n",
      "   macro avg       0.75      0.60      0.63     50817\n",
      "weighted avg       0.79      0.80      0.76     50817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "spacy_tags2 = spacy_ner(nlp2, test_tokens)\n",
    "spacy_tags_conv2 = convert_spacy_conll(spacy_tags2)\n",
    "\n",
    "# Afficher le rapport d'évaluation de classification\n",
    "print(\"Rapport d'évaluation de classification :\")\n",
    "print(classification_report(spacy_tags_conv2, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion finale\n",
    "\n",
    "Veuillez comparer les scores des trois modèles testés, en termes de **macro avg**.  Pourquoi ce score est-il le plus informatif ?  Veuillez indiquer également la taille des modèles spaCy évalués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne macro pour chaque modèle :\n",
      "+------------------------+-----------+--------+----------+\n",
      "|         Modele         | Precision | recall | F1-score |\n",
      "+------------------------+-----------+--------+----------+\n",
      "|          NLTK          |   0.56    |  0.59  |   0.56   |\n",
      "| Spacy (en_core_web_sm) |    0.7    |  0.57  |   0.6    |\n",
      "| Spacy (en_core_web_lg) |   0.75    |  0.6   |   0.63   |\n",
      "+------------------------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(\"Moyenne macro pour chaque modèle :\")\n",
    "print(tabulate([['NLTK', 0.56, 0.59, 0.56],\n",
    "                ['Spacy (en_core_web_sm)', 0.70, 0.57, 0.60],\n",
    "                [\"Spacy (en_core_web_lg)\", 0.75, 0.60, 0.63]],\n",
    "        headers=[\"Modele\", 'Precision', 'recall', \"F1-score\"], tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut voir que le modele de spacy avec le modele en_core_web_lg est le meilleur modele.\n",
    "# Il a une precision de 0.75, un recall de 0.60 et un F1-score de 0.63.\n",
    "# Les moyennes macro sont nous parlent plus que les moyennes micros dans ce cas-ci, car les tags sont déséquilibrées. \n",
    "# Les \"O\" et \"I-MISC\" sont beacoup plus fréquents que les autres tags. Surtout les \"O\" qui sont 10 fois plus fréquents que les autres tags.\n",
    "# Les moyenne micro represente ici majoritairement la performance des modeles sur les \"O\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
